## Brief History of Stability Theory

### Year 100 B.C



Aristole reaches down and takes a fistful of dirt from the ground. Standing back up, he holds his fist in front of young Alexander the Great's face for a moment before opening his hand and letting the dirt fall.

"You see Alex? The dirt falls to the ground because it is made of the same substance. Just as water will seek the ocean downstream. Just as fire will reach for the sun."

### Year 2020 A.C

The tired physics graduate student stands at the whiteboard. Introduction to Physics for the Life Sciences. 9am. She clears her throat and starts talking.

"OK class, if I take this ball and put it at the bottom of this well and then flick it gently--what will happen?"

The group of freshman in the back look at her for a moment and then go back to gossiping about something from the night before. A girl in the front row pops pink bubblegum as she texts under her desk. 

"Right. Well, the ball will eventually settle back down to the bottom of the well. Can anyone tell me why?"

Looking up from her phone, the girl offers:

"Uhmmm gravity?" 

"Yes that's right! More specifically if we write down Newton's laws and then linearize around the fixed point at the bottom of the potential well and compute the eigenvalues of the Jacobian, we can see that all the eigenvalues live in the left-hand side of the complex plane and therefore we know from Lyapunov that..."

----

Stability theory has come a long way since Aristole. It is a powerful formal structure for understanding complex dynamical systems. Since it's used by engineers out in the world where the risks are high (e.g building bridges) it is nessesarily a math-heavy, exact theory. In the case of linear dynamical systems, this theory is basically complete. In the case of nonlinear dynamical systems, we have almost no theory at all. Stability theory has played a pivatol, if under-ackonwledged role in the historical development of neuroscience. Although one could start much earlier, our story begins in the early 20th century with the rise of cybernetics. 


For fun, I'll try take their old notation and convert it into something modern. Here's an inline formula: $$e^{\pi i} + 1 = 0$$   and here's not:

$$\begin{pmatrix}
   a & b \\
   c & d
\end{pmatrix}$$


# Experimental 
As soon as people realized neurons were stochastic, they asked how percepts could be stable. 

# First Glimmers, Cybernetics 
Nobert Weiner 
Ross Ashby 
Soviet School, Learning Rules

# Biophysics
Hodgkin-Huxley

# Neural Networks
Perceptron
Learning Rule Stability 

# Between First and Second AI Winter
Hopfield 
Cohen Grossberg
SOFM 
Gradient Descent, Backprop Etc




